{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episode finished after 12.000000 time steps / mean 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uu/.pyenv/versions/3.8.2/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Episode finished after 1.000000 time steps / mean -1.890000\n",
      "2 Episode finished after 8.000000 time steps / mean -3.890000\n",
      "3 Episode finished after 13.000000 time steps / mean -5.820000\n",
      "4 Episode finished after 37.000000 time steps / mean -7.700000\n",
      "5 Episode finished after 8.000000 time steps / mean -9.340000\n",
      "6 Episode finished after 9.000000 time steps / mean -11.270000\n",
      "7 Episode finished after 16.000000 time steps / mean -13.190000\n",
      "8 Episode finished after 12.000000 time steps / mean -15.040000\n",
      "9 Episode finished after 15.000000 time steps / mean -16.930000\n",
      "10 Episode finished after 13.000000 time steps / mean -18.790000\n",
      "11 Episode finished after 131.000000 time steps / mean -20.670000\n",
      "12 Episode finished after 10.000000 time steps / mean -21.370000\n",
      "13 Episode finished after 16.000000 time steps / mean -23.280000\n",
      "14 Episode finished after 9.000000 time steps / mean -25.130000\n",
      "15 Episode finished after 17.000000 time steps / mean -27.050000\n",
      "16 Episode finished after 13.000000 time steps / mean -28.890000\n",
      "17 Episode finished after 116.000000 time steps / mean -30.770000\n",
      "18 Episode finished after 166.000000 time steps / mean -31.620000\n",
      "19 Episode finished after 66.000000 time steps / mean -31.970000\n",
      "20 Episode finished after 170.000000 time steps / mean -33.320000\n",
      "21 Episode finished after 171.000000 time steps / mean -33.630000\n",
      "22 Episode finished after 103.000000 time steps / mean -33.930000\n",
      "23 Episode finished after 115.000000 time steps / mean -34.910000\n",
      "24 Episode finished after 148.000000 time steps / mean -35.770000\n",
      "25 Episode finished after 96.000000 time steps / mean -36.300000\n",
      "26 Episode finished after 200.000000 time steps / mean -37.350000\n",
      "27 Episode finished after 138.000000 time steps / mean -35.350000\n",
      "28 Episode finished after 200.000000 time steps / mean -35.980000\n",
      "29 Episode finished after 179.000000 time steps / mean -33.980000\n",
      "30 Episode finished after 186.000000 time steps / mean -34.200000\n",
      "31 Episode finished after 200.000000 time steps / mean -34.350000\n",
      "32 Episode finished after 87.000000 time steps / mean -32.350000\n",
      "33 Episode finished after 65.000000 time steps / mean -33.490000\n",
      "34 Episode finished after 77.000000 time steps / mean -34.850000\n",
      "35 Episode finished after 49.000000 time steps / mean -36.090000\n",
      "36 Episode finished after 95.000000 time steps / mean -37.610000\n",
      "37 Episode finished after 95.000000 time steps / mean -38.670000\n",
      "38 Episode finished after 200.000000 time steps / mean -39.730000\n",
      "39 Episode finished after 200.000000 time steps / mean -37.730000\n",
      "40 Episode finished after 200.000000 time steps / mean -35.730000\n",
      "41 Episode finished after 200.000000 time steps / mean -33.730000\n",
      "42 Episode finished after 200.000000 time steps / mean -31.730000\n",
      "43 Episode finished after 171.000000 time steps / mean -29.730000\n",
      "44 Episode finished after 200.000000 time steps / mean -30.030000\n",
      "45 Episode finished after 92.000000 time steps / mean -28.030000\n",
      "46 Episode finished after 60.000000 time steps / mean -29.120000\n",
      "47 Episode finished after 87.000000 time steps / mean -30.530000\n",
      "48 Episode finished after 106.000000 time steps / mean -31.670000\n",
      "49 Episode finished after 165.000000 time steps / mean -32.620000\n",
      "50 Episode finished after 64.000000 time steps / mean -32.980000\n",
      "51 Episode finished after 73.000000 time steps / mean -34.350000\n",
      "52 Episode finished after 48.000000 time steps / mean -35.630000\n",
      "53 Episode finished after 42.000000 time steps / mean -37.160000\n",
      "54 Episode finished after 163.000000 time steps / mean -38.750000\n",
      "55 Episode finished after 67.000000 time steps / mean -39.130000\n",
      "56 Episode finished after 157.000000 time steps / mean -40.470000\n",
      "57 Episode finished after 135.000000 time steps / mean -40.910000\n",
      "58 Episode finished after 70.000000 time steps / mean -41.570000\n",
      "59 Episode finished after 78.000000 time steps / mean -42.880000\n",
      "60 Episode finished after 200.000000 time steps / mean -44.110000\n",
      "61 Episode finished after 134.000000 time steps / mean -42.110000\n",
      "62 Episode finished after 200.000000 time steps / mean -42.780000\n",
      "63 Episode finished after 200.000000 time steps / mean -40.780000\n",
      "64 Episode finished after 200.000000 time steps / mean -38.780000\n",
      "65 Episode finished after 200.000000 time steps / mean -36.780000\n",
      "66 Episode finished after 200.000000 time steps / mean -34.780000\n",
      "67 Episode finished after 200.000000 time steps / mean -32.780000\n",
      "68 Episode finished after 200.000000 time steps / mean -30.780000\n",
      "69 Episode finished after 200.000000 time steps / mean -28.780000\n",
      "70 Episode finished after 200.000000 time steps / mean -26.780000\n",
      "71 Episode finished after 200.000000 time steps / mean -24.780000\n",
      "72 Episode finished after 200.000000 time steps / mean -22.780000\n",
      "73 Episode finished after 200.000000 time steps / mean -20.780000\n",
      "74 Episode finished after 65.000000 time steps / mean -18.780000\n",
      "75 Episode finished after 48.000000 time steps / mean -20.140000\n",
      "76 Episode finished after 171.000000 time steps / mean -21.670000\n",
      "77 Episode finished after 101.000000 time steps / mean -21.970000\n",
      "78 Episode finished after 88.000000 time steps / mean -22.970000\n",
      "79 Episode finished after 200.000000 time steps / mean -24.100000\n",
      "80 Episode finished after 200.000000 time steps / mean -22.100000\n",
      "81 Episode finished after 200.000000 time steps / mean -20.100000\n",
      "82 Episode finished after 200.000000 time steps / mean -18.100000\n",
      "83 Episode finished after 192.000000 time steps / mean -16.100000\n",
      "84 Episode finished after 200.000000 time steps / mean -16.190000\n",
      "85 Episode finished after 200.000000 time steps / mean -14.190000\n",
      "86 Episode finished after 200.000000 time steps / mean -12.190000\n",
      "87 Episode finished after 200.000000 time steps / mean -10.190000\n",
      "88 Episode finished after 200.000000 time steps / mean -8.190000\n",
      "89 Episode finished after 200.000000 time steps / mean -6.190000\n",
      "90 Episode finished after 200.000000 time steps / mean -4.190000\n",
      "91 Episode finished after 200.000000 time steps / mean -2.190000\n",
      "92 Episode finished after 200.000000 time steps / mean -0.190000\n",
      "93 Episode finished after 200.000000 time steps / mean 1.810000\n",
      "94 Episode finished after 200.000000 time steps / mean 3.810000\n",
      "95 Episode finished after 200.000000 time steps / mean 5.810000\n",
      "96 Episode finished after 200.000000 time steps / mean 7.810000\n",
      "97 Episode finished after 200.000000 time steps / mean 9.810000\n",
      "98 Episode finished after 200.000000 time steps / mean 11.810000\n",
      "99 Episode finished after 200.000000 time steps / mean 13.810000\n",
      "100 Episode finished after 200.000000 time steps / mean 15.810000\n",
      "101 Episode finished after 200.000000 time steps / mean 19.700000\n",
      "102 Episode finished after 200.000000 time steps / mean 23.700000\n",
      "103 Episode finished after 200.000000 time steps / mean 27.630000\n",
      "104 Episode finished after 200.000000 time steps / mean 31.510000\n",
      "105 Episode finished after 200.000000 time steps / mean 35.150000\n",
      "106 Episode finished after 200.000000 time steps / mean 39.080000\n",
      "107 Episode finished after 200.000000 time steps / mean 43.000000\n",
      "108 Episode finished after 200.000000 time steps / mean 46.850000\n",
      "109 Episode finished after 200.000000 time steps / mean 50.740000\n",
      "110 Episode finished after 200.000000 time steps / mean 54.600000\n",
      "111 Episode finished after 200.000000 time steps / mean 58.480000\n",
      "112 Episode finished after 200.000000 time steps / mean 61.180000\n",
      "113 Episode finished after 200.000000 time steps / mean 65.090000\n",
      "114 Episode finished after 200.000000 time steps / mean 68.940000\n",
      "115 Episode finished after 200.000000 time steps / mean 72.860000\n",
      "116 Episode finished after 200.000000 time steps / mean 76.700000\n",
      "117 Episode finished after 200.000000 time steps / mean 80.580000\n",
      "118 Episode finished after 200.000000 time steps / mean 83.430000\n",
      "119 Episode finished after 200.000000 time steps / mean 85.780000\n",
      "120 Episode finished after 200.000000 time steps / mean 89.130000\n",
      "121 Episode finished after 200.000000 time steps / mean 91.440000\n",
      "122 Episode finished after 200.000000 time steps / mean 93.740000\n",
      "123 Episode finished after 200.000000 time steps / mean 96.720000\n",
      "124 Episode finished after 200.000000 time steps / mean 99.580000\n",
      "125 Episode finished after 200.000000 time steps / mean 102.110000\n",
      "126 Episode finished after 200.000000 time steps / mean 105.160000\n",
      "127 Episode finished after 200.000000 time steps / mean 105.160000\n",
      "128 Episode finished after 200.000000 time steps / mean 107.790000\n",
      "129 Episode finished after 200.000000 time steps / mean 107.790000\n",
      "130 Episode finished after 200.000000 time steps / mean 110.010000\n",
      "131 Episode finished after 200.000000 time steps / mean 112.160000\n",
      "132 Episode finished after 200.000000 time steps / mean 112.160000\n",
      "133 Episode finished after 200.000000 time steps / mean 115.300000\n",
      "134 Episode finished after 200.000000 time steps / mean 118.660000\n",
      "135 Episode finished after 200.000000 time steps / mean 121.900000\n",
      "136 Episode finished after 200.000000 time steps / mean 125.420000\n",
      "137 Episode finished after 200.000000 time steps / mean 128.480000\n",
      "138 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "139 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "140 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "141 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "142 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "143 Episode finished after 200.000000 time steps / mean 131.540000\n",
      "144 Episode finished after 200.000000 time steps / mean 133.840000\n",
      "145 Episode finished after 200.000000 time steps / mean 133.840000\n",
      "146 Episode finished after 200.000000 time steps / mean 136.930000\n",
      "147 Episode finished after 200.000000 time steps / mean 140.340000\n",
      "148 Episode finished after 200.000000 time steps / mean 143.480000\n",
      "149 Episode finished after 200.000000 time steps / mean 146.430000\n",
      "150 Episode finished after 200.000000 time steps / mean 148.790000\n",
      "151 Episode finished after 200.000000 time steps / mean 152.160000\n",
      "152 Episode finished after 200.000000 time steps / mean 155.440000\n",
      "153 Episode finished after 200.000000 time steps / mean 158.970000\n",
      "154 Episode finished after 200.000000 time steps / mean 162.560000\n",
      "155 Episode finished after 200.000000 time steps / mean 164.940000\n",
      "156 Episode finished after 200.000000 time steps / mean 168.280000\n",
      "157 Episode finished after 200.000000 time steps / mean 170.720000\n",
      "158 Episode finished after 200.000000 time steps / mean 173.380000\n",
      "159 Episode finished after 200.000000 time steps / mean 176.690000\n",
      "160 Episode finished after 200.000000 time steps / mean 179.920000\n",
      "161 Episode finished after 200.000000 time steps / mean 179.920000\n",
      "162 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "163 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "164 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "165 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "166 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "167 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "168 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "169 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "170 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "171 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "172 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "173 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "174 Episode finished after 200.000000 time steps / mean 182.590000\n",
      "175 Episode finished after 200.000000 time steps / mean 185.950000\n",
      "176 Episode finished after 200.000000 time steps / mean 189.480000\n",
      "177 Episode finished after 200.000000 time steps / mean 191.780000\n",
      "178 Episode finished after 200.000000 time steps / mean 194.780000\n",
      "Episode 178 train agent successfuly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 Episode finished after 1.000000 time steps / mean 197.910000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "# [0]ライブラリのインポート\n",
    "import gym  #倒立振子(cartpole)の実行環境\n",
    "from gym import wrappers  #gymの画像保存\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs(\"gif\", exist_ok=True)\n",
    "\n",
    "\n",
    "# [1]Q関数を離散化して定義する関数　------------\n",
    "# 観測した状態を離散値にデジタル変換する\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    " \n",
    "# 各値を離散値に変換\n",
    "def digitize_state(observation):\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n",
    "\n",
    "\n",
    "# [2]行動a(t)を求める関数 -------------------------------------\n",
    "def get_action(next_state, episode):\n",
    "           #徐々に最適行動のみをとる、ε-greedy法\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action\n",
    "\n",
    "\n",
    "# [3]Qテーブルを更新する関数 -------------------------------------\n",
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.99\n",
    "    alpha = 0.5\n",
    "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
    "            alpha * (reward + gamma * next_Max_Q)\n",
    "   \n",
    "    return q_table\n",
    " \n",
    "# [4]. メイン関数開始 パラメータ設定--------------------------------------------------------\n",
    "env = gym.make('CartPole-v0')\n",
    "max_number_of_steps = 200  #1試行のstep数\n",
    "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n",
    "num_episodes = 2000  #総試行回数\n",
    "goal_average_reward = 195  #この報酬を超えると学習終了（中心への制御なし）\n",
    "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\n",
    "num_dizitized = 6  #分割数\n",
    "q_table = np.random.uniform(\n",
    "    low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\n",
    " \n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\n",
    "final_x = np.zeros((num_episodes, 1))  #学習後、各試行のt=200でのｘの位置を格納\n",
    "islearned = 0  #学習が終わったフラグ\n",
    "isrender = 0  #描画フラグ\n",
    "issaved = 0\n",
    "\n",
    "\n",
    "# [5] メインルーチン--------------------------------------------------\n",
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    # 環境の初期化\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation)\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    "    \n",
    "    frames = []\n",
    "    for t in range(max_number_of_steps):  #1試行のループ\n",
    "#         print(frame)\n",
    "        if islearned == 1:\n",
    "            if issaved == 1:\n",
    "                break\n",
    "            for _ in range(6000):\n",
    "                frame = env.render(mode='rgb_array')\n",
    "                frames.append(frame)\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "                next_state = digitize_state(observation)\n",
    "                action = get_action(next_state, episode)\n",
    "#             issaved = 1\n",
    "#         if islearned == 1:  #学習s終了したらcartPoleを描画する\n",
    "            \n",
    "#             time.sleep(0.1)\n",
    "#             print (observation[0])  #カートのx位置を出力\n",
    " \n",
    "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
    "        observation, reward, done, info = env.step(action)\n",
    " \n",
    "        # 報酬を設定し与える\n",
    "        if done:\n",
    "            if t < 195:\n",
    "                reward = -200  #こけたら罰則\n",
    "            else:\n",
    "                reward = 1  #立ったまま終了時は罰則はなし\n",
    "        else:\n",
    "            reward = 1  #各ステップで立ってたら報酬追加\n",
    " \n",
    "        episode_reward += reward  #報酬を追加\n",
    " \n",
    "        # 離散状態s_{t+1}を求め、Q関数を更新する\n",
    "        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state)\n",
    "        \n",
    "        #  次の行動a_{t+1}を求める \n",
    "        action = get_action(next_state, episode)    # a_{t+1} \n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        #終了時の処理\n",
    "        if done:\n",
    "            print('%d Episode finished after %f time steps / mean %f' %\n",
    "                  (episode, t + 1, total_reward_vec.mean()))\n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:],\n",
    "                                          episode_reward))  #報酬を記録\n",
    "            if islearned == 1:  #学習終わってたら最終のx座標を格納\n",
    "                final_x[episode, 0] = observation[0]\n",
    "            break\n",
    "\n",
    "    if islearned == 1:\n",
    "        if issaved ==1:\n",
    "            break\n",
    "        save_frames_as_gif(frames, path=\"gif/\", filename=f\"{str(episode).zfill(3)}_.gif\")\n",
    "        issaved = 1\n",
    "    if (total_reward_vec.mean() >=\n",
    "            goal_average_reward):  # 直近の100エピソードが規定報酬以上であれば成功\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        islearned = 1\n",
    "        #np.savetxt('learned_Q_table.csv',q_table, delimiter=\",\") #Qtableの保存する場合\n",
    "        if isrender == 0:\n",
    "            #env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "            isrender = 1\n",
    "    #10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
    "    #if episode>10:\n",
    "    #    if isrender == 0:\n",
    "    #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "    #        isrender = 1\n",
    "    #    islearned=1;\n",
    "\n",
    "# if islearned:\n",
    "#     np.savetxt('final_x.csv', final_x, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import gym \n",
    "\n",
    "\"\"\"\n",
    "Ensure you have imagemagick installed with \n",
    "sudo apt-get install imagemagick\n",
    "Open file in CLI with:\n",
    "xgd-open <filelname>\n",
    "\"\"\"\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "\n",
    "    #Mess with this to change frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    anim.save(path + filename, writer='imagemagick', fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for _ in range(1000):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
